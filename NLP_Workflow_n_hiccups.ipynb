{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `NLP WorkFlow and Problems That Arise`:\n",
    "\n",
    "# <font color=red>Mr Fugu Data Science</font>\n",
    "\n",
    "# (◕‿◕✿)\n",
    "\n",
    "# Purpose & Outcome:\n",
    "\n",
    "+ General Workflow of an actual example\n",
    "\n",
    "+ Show what problems come up and how you need to think about this depending on your analysis\n",
    "\n",
    "`*********************************`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, data is unstructured and needs to be converted or interpreted into a certain format for analysis. Textual data is no exception; evaluating social media posts or short messages such as text messages (sms) can be difficult tasks to handle due lack of structure, slang, etc.\n",
    "\n",
    "`This dataset came from the https://newsapi.org/ on Sept. 19,2020`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsides:\n",
    "\n",
    "+ There are situations where `stemming` and `lemmatizing` are not useful and can effect your analysis. You need to consider when this may be a problem:\n",
    "\n",
    "for example, if we were looking that getting a gist of a news article. Say that we want to summarize the article in just a few words then the part of speech (POS) is very important. Changing the ending of a word or changing to lowercase can be a problem.\n",
    "\n",
    "+ Having spelling errors: \n",
    "\n",
    "+ Words not separated by space `supposebut`, `wearehungry`\n",
    "\n",
    "+ Conjugation: `don't`, `can't`\n",
    "\n",
    "+ Abreviations/Acronyms: `gov't`,`FBI`,`NASA`\n",
    "\n",
    "These are all concerns that need to be handled on a case by case basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk                              # text processing\n",
    "from nltk import word_tokenize           # split sentence into list of words\n",
    "from nltk.corpus import stopwords        # remove: and,it,i,etc\n",
    "import string                            # punctuation removal\n",
    "from nltk.stem import WordNetLemmatizer  # remove word endings etc\n",
    "from collections import defaultdict      # dict with values as lists\n",
    "from textblob import TextBlob            # fix spelling \n",
    "from nltk.corpus import words            # find words as comparison to see if valid\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA declined to comment to Engadget. We’ve also asked SoftBank and ARM for comment.\r\n",
      "If a deal goes through, it could represents one of the largest semiconductor buyouts in history. It potentially… [+694 chars]\n",
      "----------------------\n",
      "SoftBank might be close to finding a buyer for ARM, and it won’t surprise you who the bidder might be. Wall Street Journal sources say SoftBank is close to a deal to sell ARM Holdings to NVIDIA for “more than” $40 billion. The two have reportedly been in excl…\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>Brendan Hesse</td>\n",
       "      <td>Everything You Missed From Today's Facebook Co...</td>\n",
       "      <td>Today’s Facebook Connect kicked off with a two...</td>\n",
       "      <td>https://lifehacker.com/everything-you-missed-f...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>2020-09-16 20:30:00</td>\n",
       "      <td>Todays Facebook Connect kicked off with a two-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>Joel Cunningham</td>\n",
       "      <td>How to Turn Off Alexa's Creepy 'Whisper Mode'</td>\n",
       "      <td>I love my smart speaker—as much as one can eve...</td>\n",
       "      <td>https://lifehacker.com/how-to-turn-off-alexas-...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>2020-08-31 20:30:00</td>\n",
       "      <td>I love my smart speakeras much as one can ever...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           source           author  \\\n",
       "0  Lifehacker.com    Brendan Hesse   \n",
       "1  Lifehacker.com  Joel Cunningham   \n",
       "\n",
       "                                               title  \\\n",
       "0  Everything You Missed From Today's Facebook Co...   \n",
       "1      How to Turn Off Alexa's Creepy 'Whisper Mode'   \n",
       "\n",
       "                                         description  \\\n",
       "0  Today’s Facebook Connect kicked off with a two...   \n",
       "1  I love my smart speaker—as much as one can eve...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://lifehacker.com/everything-you-missed-f...   \n",
       "1  https://lifehacker.com/how-to-turn-off-alexas-...   \n",
       "\n",
       "                                          urlToImage          publishedAt  \\\n",
       "0  https://i.kinja-img.com/gawker-media/image/upl...  2020-09-16 20:30:00   \n",
       "1  https://i.kinja-img.com/gawker-media/image/upl...  2020-08-31 20:30:00   \n",
       "\n",
       "                                             content  \n",
       "0  Todays Facebook Connect kicked off with a two-...  \n",
       "1  I love my smart speakeras much as one can ever...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# news file to parse:\n",
    "news_=pd.read_csv('goog_api_techstuff.csv')\n",
    "\n",
    "# one entry: which appears to be a small article\n",
    "print(news_['content'][13])\n",
    "print('----------------------')\n",
    "print(news_['description'][13])\n",
    "\n",
    "news_.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Convert to lowercase, remove punctuation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['todays',\n",
       " 'facebook',\n",
       " 'connect',\n",
       " 'kicked',\n",
       " 'off',\n",
       " 'with',\n",
       " 'a',\n",
       " 'twohour',\n",
       " 'keynote',\n",
       " 'that',\n",
       " 'detailed',\n",
       " 'the',\n",
       " 'companys',\n",
       " 'latest',\n",
       " 'virtual',\n",
       " 'and',\n",
       " 'augmented',\n",
       " 'reality',\n",
       " 'developments',\n",
       " 'there',\n",
       " 'were',\n",
       " 'product',\n",
       " 'announcements',\n",
       " 'new',\n",
       " 'apps',\n",
       " 'and',\n",
       " 'games',\n",
       " 'on',\n",
       " 'displa…',\n",
       " '5586',\n",
       " 'chars']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wrd_lst_tokens=[]\n",
    "for ikl in news_['content']:\n",
    "    punct=word_tokenize(''.join(j for j in ikl.lower() if j not in string.punctuation))\n",
    "    wrd_lst_tokens.append([punct])\n",
    "\n",
    "pd.DataFrame(wrd_lst_tokens).head()\n",
    "\n",
    "pd.DataFrame(wrd_lst_tokens)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same above code but modified: `remove digits and replace hyphens`\n",
    "\n",
    "+ if I don't remove hyphens and replace with empty string you will get a conjunction or combined wording. Which I don't want\n",
    "\n",
    "+ The digits are removed because, \n",
    "\n",
    "    1. ) the end of each string is only a representation of the total article and it shows the remaining portion in terms of character length. \n",
    "    \n",
    "    2. ) I do not think I need them for this exercise and show how to deal with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['todays',\n",
       " 'facebook',\n",
       " 'connect',\n",
       " 'kicked',\n",
       " 'off',\n",
       " 'with',\n",
       " 'a',\n",
       " 'two',\n",
       " 'hour',\n",
       " 'keynote',\n",
       " 'that',\n",
       " 'detailed',\n",
       " 'the',\n",
       " 'companys',\n",
       " 'latest',\n",
       " 'virtual',\n",
       " 'and',\n",
       " 'augmented',\n",
       " 'reality',\n",
       " 'developments',\n",
       " 'there',\n",
       " 'were',\n",
       " 'product',\n",
       " 'announcements',\n",
       " 'new',\n",
       " 'apps',\n",
       " 'and',\n",
       " 'games',\n",
       " 'on',\n",
       " 'displa…',\n",
       " 'chars']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrd_lst_tokens_updt=[]\n",
    "for ikl in news_['content']:\n",
    "    stopwrds = stopwords.words('english')\n",
    "\n",
    "    punct=word_tokenize(''.join(j for j in ikl.replace('-',' ').lower()\\\n",
    "                                if j not in string.punctuation if not j.isdigit()))\n",
    "    \n",
    "    wrd_lst_tokens_updt.append([punct])\n",
    "\n",
    "\n",
    "pd.DataFrame(wrd_lst_tokens_updt)[0][0]\n",
    "# .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Remove Stopwords, single characters, useless words as well`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of amount of data after:  0.09813084112149532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['evo',\n",
       " 'promise',\n",
       " 'hours',\n",
       " 'battery',\n",
       " 'life',\n",
       " 'plus',\n",
       " 'rapid',\n",
       " 'charging',\n",
       " 'technology',\n",
       " 'delivers',\n",
       " 'four',\n",
       " 'hour',\n",
       " 'boost',\n",
       " 'minute',\n",
       " 'charge',\n",
       " 'touch',\n",
       " 'controls',\n",
       " 'let',\n",
       " 'users',\n",
       " 'take',\n",
       " 'calls',\n",
       " 'change',\n",
       " 'tracks',\n",
       " 'adjust',\n",
       " 'volume']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=[]\n",
    "for i in wrd_lst_tokens_updt:\n",
    "# removing single letters, and [:-3] will remove last 3 str from each list bc useless\n",
    "    line = [j for j in i[0][:-2] if len(j) > 1]\n",
    "    \n",
    "# remove our stopwords like: ('i','it','etc')\n",
    "    d.append([[ii for ii in line if ii not in stopwrds]])\n",
    "\n",
    "\n",
    "print('Example of amount of data after: ',len(pd.DataFrame(d)[0][1])/len(news_['content'][0]))\n",
    "pd.DataFrame(d).head()\n",
    "pd.DataFrame(d)[0][8]\n",
    "# pd.DataFrame(d)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Lemmatization`: looking for root or bases of words\n",
    "\n",
    "Now, you can use `Stemming` if you are pressed for time or memory. Because, you will just chop off endings of words like: 's','ies' etc.\n",
    "\n",
    "Using a `Lemma` you are using a rule based approach and working more succinctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  :  The\n",
      "striped  :  striped\n",
      "bats  :  bat\n",
      "are  :  are\n",
      "hanging  :  hanging\n",
      "on  :  on\n",
      "their  :  their\n",
      "feet  :  foot\n",
      "for  :  for\n",
      "best  :  best\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "words = word_tokenize(sentence)\n",
    "for w in words:\n",
    "    print(w, \" : \", lemmatizer.lemmatize(w))\n",
    "\n",
    "# This was from stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>urlToImage</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>content</th>\n",
       "      <th>updated_strings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>Brendan Hesse</td>\n",
       "      <td>Everything You Missed From Today's Facebook Co...</td>\n",
       "      <td>Today’s Facebook Connect kicked off with a two...</td>\n",
       "      <td>https://lifehacker.com/everything-you-missed-f...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>2020-09-16 20:30:00</td>\n",
       "      <td>Todays Facebook Connect kicked off with a two-...</td>\n",
       "      <td>[today, facebook, connect, kicked, two, hour, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>Joel Cunningham</td>\n",
       "      <td>How to Turn Off Alexa's Creepy 'Whisper Mode'</td>\n",
       "      <td>I love my smart speaker—as much as one can eve...</td>\n",
       "      <td>https://lifehacker.com/how-to-turn-off-alexas-...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>2020-08-31 20:30:00</td>\n",
       "      <td>I love my smart speakeras much as one can ever...</td>\n",
       "      <td>[love, smart, speakeras, much, one, ever, love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>Elizabeth Yuko</td>\n",
       "      <td>How to Avoid Getting a Last-Minute Booking Blo...</td>\n",
       "      <td>Airbnb is cracking down on parties. They are n...</td>\n",
       "      <td>https://lifehacker.com/how-to-avoid-getting-a-...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>2020-09-06 14:00:00</td>\n",
       "      <td>Airbnb is cracking down on parties. They are n...</td>\n",
       "      <td>[airbnb, cracking, party, basically, equivalen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>Beth Skwarecki on Vitals, shared by Beth Skwar...</td>\n",
       "      <td>Tackle a Hill Head-On</td>\n",
       "      <td>Did you find a new trail to run or hike for la...</td>\n",
       "      <td>https://vitals.lifehacker.com/tackle-a-hill-he...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>2020-09-11 16:30:00</td>\n",
       "      <td>Did you find a new trail to run or hike for la...</td>\n",
       "      <td>[find, new, trail, run, hike, last, week, chal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lifehacker.com</td>\n",
       "      <td>Meghan Moravcik Walbert on Offspring, shared b...</td>\n",
       "      <td>How to Communicate With Kids When You're Weari...</td>\n",
       "      <td>“Mask-muffle” is a term I just made up, but it...</td>\n",
       "      <td>https://offspring.lifehacker.com/how-to-commun...</td>\n",
       "      <td>https://i.kinja-img.com/gawker-media/image/upl...</td>\n",
       "      <td>2020-09-17 13:00:00</td>\n",
       "      <td>Mask-muffle is a term I just made up, but its ...</td>\n",
       "      <td>[mask, muffle, term, made, real, thing, cloth,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           source                                             author  \\\n",
       "0  Lifehacker.com                                      Brendan Hesse   \n",
       "1  Lifehacker.com                                    Joel Cunningham   \n",
       "2  Lifehacker.com                                     Elizabeth Yuko   \n",
       "3  Lifehacker.com  Beth Skwarecki on Vitals, shared by Beth Skwar...   \n",
       "4  Lifehacker.com  Meghan Moravcik Walbert on Offspring, shared b...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Everything You Missed From Today's Facebook Co...   \n",
       "1      How to Turn Off Alexa's Creepy 'Whisper Mode'   \n",
       "2  How to Avoid Getting a Last-Minute Booking Blo...   \n",
       "3                              Tackle a Hill Head-On   \n",
       "4  How to Communicate With Kids When You're Weari...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Today’s Facebook Connect kicked off with a two...   \n",
       "1  I love my smart speaker—as much as one can eve...   \n",
       "2  Airbnb is cracking down on parties. They are n...   \n",
       "3  Did you find a new trail to run or hike for la...   \n",
       "4  “Mask-muffle” is a term I just made up, but it...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://lifehacker.com/everything-you-missed-f...   \n",
       "1  https://lifehacker.com/how-to-turn-off-alexas-...   \n",
       "2  https://lifehacker.com/how-to-avoid-getting-a-...   \n",
       "3  https://vitals.lifehacker.com/tackle-a-hill-he...   \n",
       "4  https://offspring.lifehacker.com/how-to-commun...   \n",
       "\n",
       "                                          urlToImage          publishedAt  \\\n",
       "0  https://i.kinja-img.com/gawker-media/image/upl...  2020-09-16 20:30:00   \n",
       "1  https://i.kinja-img.com/gawker-media/image/upl...  2020-08-31 20:30:00   \n",
       "2  https://i.kinja-img.com/gawker-media/image/upl...  2020-09-06 14:00:00   \n",
       "3  https://i.kinja-img.com/gawker-media/image/upl...  2020-09-11 16:30:00   \n",
       "4  https://i.kinja-img.com/gawker-media/image/upl...  2020-09-17 13:00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  Todays Facebook Connect kicked off with a two-...   \n",
       "1  I love my smart speakeras much as one can ever...   \n",
       "2  Airbnb is cracking down on parties. They are n...   \n",
       "3  Did you find a new trail to run or hike for la...   \n",
       "4  Mask-muffle is a term I just made up, but its ...   \n",
       "\n",
       "                                     updated_strings  \n",
       "0  [today, facebook, connect, kicked, two, hour, ...  \n",
       "1  [love, smart, speakeras, much, one, ever, love...  \n",
       "2  [airbnb, cracking, party, basically, equivalen...  \n",
       "3  [find, new, trail, run, hike, last, week, chal...  \n",
       "4  [mask, muffle, term, made, real, thing, cloth,...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h=[]\n",
    "for i in range(len(d)):\n",
    "    for j in d[i][0]:\n",
    "#         print(j)\n",
    "        h.append([i,lemmatizer.lemmatize(j)])\n",
    "\n",
    "dg=defaultdict(list)\n",
    "for i in h:\n",
    "    dg[i[0]].append(i[1])\n",
    "\n",
    "news_['updated_strings']=dg.values()\n",
    "news_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Misspellings with TextBlob:\n",
    "\n",
    "+ This is not perfect let's look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Version:  ['love', 'smart', 'speakers', 'much', 'one', 'ever', 'love', 'piece', 'privacy', 'stealing', 'technology', 'exists', 'gather', 'information', 'supposebut', 'doesn', 'mean', 'dont', 'find', 'many', 'thing']\n",
      "-----------------\n",
      "Original:  ['love', 'smart', 'speakeras', 'much', 'one', 'ever', 'love', 'piece', 'privacy', 'stealing', 'technology', 'exists', 'gather', 'information', 'supposebut', 'doesnt', 'mean', 'dont', 'find', 'many', 'thing']\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "misspelled=[\"hapenning\", \"mornin\", \"windoow\", \"jaket\"]\n",
    "miss_=dg[1]\n",
    "print('Corrected Version: ',[str(TextBlob(word).correct()) for word in miss_])\n",
    "print('-----------------')\n",
    "print('Original: ',dg[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is to take care of Words not separated by a space:\n",
    "\n",
    "+ `This is Not Perfect`\n",
    "\n",
    "Code for the cell below: https://stackoverflow.com/questions/38125281/split-sentence-without-space-in-python-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acquire', 'customer', 'data']\n",
      "['suppose', 'but']\n",
      "['speaker', 'as']\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import division\n",
    "# from collections import Counter\n",
    "# import re, nltk\n",
    "\n",
    "WORDS = nltk.corpus.brown.words()\n",
    "COUNTS = Counter(WORDS)\n",
    "\n",
    "def pdist(counter):\n",
    "    \"Make a probability distribution, given evidence from a Counter.\"\n",
    "    N = sum(counter.values())\n",
    "    return lambda x: counter[x]/N\n",
    "\n",
    "P = pdist(COUNTS)\n",
    "\n",
    "def Pwords(words):\n",
    "    \"Probability of words, assuming each word is independent of others.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Multiply the numbers together.  (Like `sum`, but with multiplication.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result\n",
    "\n",
    "def splits(text, start=0, L=20):\n",
    "    \"Return a list of all (first, rest) pairs; start <= len(first) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]\n",
    "\n",
    "def segment(text):\n",
    "    \"Return a list of words that is the most probable segmentation of text.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=Pwords)\n",
    "\n",
    "print(segment('acquirecustomerdata'))\n",
    "print(segment('supposebut'))\n",
    "print(segment('speakeras'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iterate through list and see if any words are combined and need to be separated\n",
    "\n",
    "+ Notice, how this fails for some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, ['today']], [0, ['face', 'book']]]\n",
      "[0, ['a', 'p', 'p', 's']]\n"
     ]
    }
   ],
   "source": [
    "b=[]\n",
    "for i in h:\n",
    "    \n",
    "    b.append([i[0],segment(i[1])])\n",
    "\n",
    "print(b[:2]) # company name that was split\n",
    "print(b[17]) # shorthand version of applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only take lists of two words:\n",
    "\n",
    "from these data it looks like sometimes there are words that are combined because someone forgot to use a space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, ['face', 'book']], [0, 0], [0, 0], [0, 0]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y=[]\n",
    "for i in b:\n",
    "#     print(i)\n",
    "    if len(i[1])==2:\n",
    "        y.append(i)\n",
    "    else:\n",
    "        y.append([i[0],0])\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Code Block has a few interesting bits:\n",
    "\n",
    "You are iterating through this, and looking for the lists of words and combining to see if it forms a legit word. \n",
    "\n",
    "+ If it doesn't, then just append the original split\n",
    "\n",
    "+ otherwise, keep it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct:  [1, ['speaker', 'as']]\n",
      "Correct:  [[1, ['suppose', 'but']], [1, 0], [1, 0], [1, 'dont']]\n",
      "Fails:  [[9, 'io'], [9, 0], [9, 'io'], [9, 'beta'], [10, ['neural', 'ink']], [10, 0], [10, 0]]\n",
      "Fails:  [12, ['sir', 'i']]\n",
      "Fails:  [[13, ['en', 'gadget']], [13, 0], [13, 0], [13, ['soft', 'bank']]]\n"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import words\n",
    "ll=[]\n",
    "for i in y:\n",
    "#     print(i)\n",
    "    if i[1]==0:\n",
    "        ll.append([i[0],0])\n",
    "    else:\n",
    "        f=''.join(i[1])\n",
    "        if (f in words.words())==False:\n",
    "            ll.append(i)\n",
    "        else:\n",
    "            ll.append([i[0],f])\n",
    "            \n",
    "print('Correct: ',ll[21])\n",
    "print('Correct: ',ll[33:37])\n",
    "print('Fails: ',ll[189:196]) # CS related and Elon Musk: neuralink\n",
    "print('Fails: ',ll[245]) # name of Siri voice recognition\n",
    "print('Fails: ',ll[254:258]) # companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see what happens with some of our words and notice how this can fail: \n",
    "\n",
    "+ Only can find correctly spelled words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['supposeas', 'F'],\n",
       " ['neuarlink', 'F'],\n",
       " ['facebook', 'F'],\n",
       " ['suppose', 'T'],\n",
       " ['supose', 'F'],\n",
       " ['dont', 'T'],\n",
       " [\"don't\", 'F']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actual_words_from_our file\n",
    "\n",
    "lst=['supposeas','neuarlink','facebook','suppose','supose','dont',\"don't\"]\n",
    "m=[]\n",
    "for i in lst:\n",
    "    if (i in words.words())==False:\n",
    "        m.append([i,'F'])\n",
    "    else:\n",
    "        m.append([i,'T'])\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`------------------`\n",
    "\n",
    "# <font color=red>LIKE</font>, Share &\n",
    "\n",
    "# <font color=red>SUB</font>scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citations & Help:\n",
    "\n",
    "# ◔̯◔\n",
    "\n",
    "https://www.kaggle.com/matleonard/text-classification\n",
    "\n",
    "https://www.kaggle.com/matleonard/word-vectors\n",
    "\n",
    "https://github.com/graykode/nlp-tutorial\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/\n",
    "\n",
    "https://towardsdatascience.com/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72\n",
    "\n",
    "https://medium.com/machine-learning-in-practice/over-200-of-the-best-machine-learning-nlp-and-python-tutorials-2018-edition-dd8cf53cb7dc\n",
    "\n",
    "https://www.kdnuggets.com/2019/01/solve-90-nlp-problems-step-by-step-guide.html\n",
    "\n",
    "https://www.kaggle.com/itratrahman/nlp-tutorial-using-python\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "\n",
    "https://medium.com/@Intellica.AI/comparison-of-different-word-embeddings-on-text-similarity-a-use-case-in-nlp-e83e08469c1c\n",
    "\n",
    "https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/\n",
    "\n",
    "https://stackabuse.com/text-summarization-with-nltk-in-python/\n",
    "\n",
    "https://medium.com/@Intellica.AI/comparison-of-different-word-embeddings-on-text-similarity-a-use-case-in-nlp-e83e08469c1c\n",
    "\n",
    "https://opendatagroup.github.io/data%20science/2019/03/21/preprocessing-text.html\n",
    "\n",
    "https://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb\n",
    "\n",
    "https://stackoverflow.com/questions/38125281/split-sentence-without-space-in-python-nltk\n",
    "\n",
    "https://towardsdatascience.com/how-i-used-natural-language-processing-to-extract-context-from-news-headlines-df2cf5181ca6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
